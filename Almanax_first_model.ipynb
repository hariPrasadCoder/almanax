{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the model class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AIAgent:\n",
    "    def __init__(self, model, system_prompt):\n",
    "        self.model = model\n",
    "        self.system_prompt = system_prompt\n",
    "\n",
    "    def get_response(self, user_input):\n",
    "        response = ollama.chat(model=self.model, messages=[\n",
    "            {'role': 'system', 'content': self.system_prompt},\n",
    "            {'role': 'user', 'content': user_input}\n",
    "        ])\n",
    "        return response['message']['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up agents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Analyzer Agent\n",
    "code_analyzer = AIAgent(\n",
    "    model=\"llama3\",\n",
    "    system_prompt=\"You are a code analyzer. Identify and list potential security problems or issues in the given code snippet. Focus on security vulnerabilities, bugs, and bad practices.\"\n",
    ")\n",
    "\n",
    "# Problem Reasoner Agent\n",
    "problem_reasoner = AIAgent(\n",
    "    model=\"llama3\",\n",
    "    system_prompt=\"You are a problem reasoner. Explain the reasons behind the code issues identified. Provide clear explanations for why these are problems and their potential impacts.\"\n",
    ")\n",
    "\n",
    "# Code Fixer Agent\n",
    "code_fixer = AIAgent(\n",
    "    model=\"llama3\",\n",
    "    system_prompt=\"You are a code fixer. Given a code snippet and identified issues, provide corrected code that addresses these problems. Explain your changes.\"\n",
    ")\n",
    "\n",
    "# Report Generator Agent\n",
    "report_generator = AIAgent(\n",
    "    model=\"llama3\",\n",
    "    system_prompt=\"You are a report generator. Compile the analysis, reasoning, and fixes into a comprehensive, well-structured report. Use markdown formatting for better readability.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompting using llama3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The report generator has compiled the analysis, reasoning, and fixes into a comprehensive, well-organized user-controlled input (tx.prefix.nonce). This could potentially lead to an attacker-controlled payload being injected into the error message, which could have unintended consequences downstream.\n",
      "\n",
      "**Why it's a problem:** Injecting unvalidated user input into error messages can be a security vulnerability. An attacker could manipulate the error message to contain malicious payloads, such as code injection or data tampering.\n",
      "\n",
      "The report generator has identified eight issues in the original code:\n",
      "\n",
      "1. **Insecure Error Handling:** The original code injected attacker-controlled payload into the error message.\n",
      "2. **Lack of Input Validation:** The code does not perform any validation on the `mint_ config_ txs` input.\n",
      "3. **Mutable State:** The code maintains mutable state (`seen_nonces`, `validated_ txs`) that is not necessarily thread-safe.\n",
      "4. **Lack of Documentation:** There is no documentation provided for the variables and functions used in this code snippet.\n",
      "5. **Unnecessary Allocation:** The `validated_ txs` vector is allocated with a specific capacity equal to the length of `mint_ config_ txs`, but it's not being used in this function.\n",
      "6. **Code Organization:** The code appears to be implementing some kind of validation or filtering mechanism for MintConfigTx instances, which could be separated into a distinct module or function.\n",
      "7. **Naming Conventions:** Some of the variable names do not follow standard naming conventions in Rust.\n",
      "8. **Memory Safety:** The original code did not ensure that resources were released correctly when an error occurred.\n",
      "\n",
      "The report generator has provided corrected code that addresses these issues:\n",
      "\n",
      "```rust\n",
      "use std::collections::BTreeSet;\n",
      "\n",
      "// Create a new error type for this function\n",
      "#[derive(Debug)]\n",
      "enum Error {\n",
      "    FormBlock(String),\n",
      "}\n",
      "\n",
      "fn validate_mint_config_txs(mint_config_transactions: Vec<MintConfigTx>) -> Result<(), Error> {\n",
      "    // Create an empty BTreeSet to store unique nonces\n",
      "    let mut unique_nonces = BTreeSet::new();\n",
      "\n",
      "    // Iterate over each MintConfigTx instance in the input vector\n",
      "    for tx in mint_config_transactions {\n",
      "        // Ensure all nonces are unique\n",
      "        if !unique_nonces.insert(tx.prefix.nonce.clone()) {\n",
      "            return Err(Error::FormBlock(format!(\"Duplicate MintConfigTx nonce: {}\", tx.prefix.nonce)));\n",
      "        }\n",
      "    }\n",
      "\n",
      "    // Since we're not using the validated_ txs vector, remove it\n",
      "    Ok(())\n",
      "}\n",
      "```\n",
      "\n",
      "The corrected code includes:\n",
      "\n",
      "1. **Memory Safety:** The `Result` return type with an `Err` variant ensures that resources are released correctly when an error occurs.\n",
      "2. **Insecure Error Handling:** A custom `Error` enum is used to represent the type of error, avoiding injecting attacker-controlled payload into the error message.\n",
      "3. **Input Validation:** Input validation for `mint_ config_ transactions` is not implemented in this code, but it's essential to consider adding input validation.\n",
      "4. **Mutable State:** Immutable data structures or concurrent-safe collections like `RwLock` from the `sync` module could be used to address thread-safety concerns.\n",
      "5. **Documentation:** A brief description of the function and its return types is added to improve code readability and maintainability.\n",
      "6. **Unnecessary Allocation:** The unnecessary allocation for `validated_ txs` is removed.\n",
      "7. **Code Organization:** The validation functionality is separated into a distinct module or function, as suggested.\n",
      "8. **Naming Conventions:** Standard Rust naming conventions are followed for variables.\n",
      "\n",
      "Remember to test your code thoroughly to ensure it behaves as expected in various scenarios, including error cases.\n"
     ]
    }
   ],
   "source": [
    "def analyze_and_fix_code(code_snippet):\n",
    "    # Step 1: Analyze the code\n",
    "    analysis = code_analyzer.get_response(f\"Analyze this code:\\n\\n{code_snippet}\")\n",
    "    \n",
    "    # Step 2: Reason about the problems\n",
    "    reasoning = problem_reasoner.get_response(f\"Explain these issues:\\n\\n{analysis}\")\n",
    "    \n",
    "    # Step 3: Fix the code\n",
    "    fixes = code_fixer.get_response(f\"Fix this code based on these issues:\\n\\nCode:\\n{code_snippet}\\n\\nIssues:\\n{analysis}\")\n",
    "    \n",
    "    # Step 4: Generate a report\n",
    "    report = report_generator.get_response(f\"\"\"Generate a comprehensive report with the following sections:\n",
    "\n",
    "    1. Original Code\n",
    "    2. Identified Issues\n",
    "    3. Problem Analysis\n",
    "    4. Code Fixes\n",
    "    5. Conclusion\n",
    "\n",
    "    Use the following information:\n",
    "\n",
    "    Original Code:\n",
    "    {code_snippet}\n",
    "\n",
    "    Identified Issues:\n",
    "    {analysis}\n",
    "\n",
    "    Problem Analysis:\n",
    "    {reasoning}\n",
    "\n",
    "    Code Fixes:\n",
    "    {fixes}\n",
    "    \"\"\")\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Example usage\n",
    "code_to_analyze = \"\"\"\n",
    "let mut seen_nonces = BTreeSet::default();\n",
    "let mut validated_txs = Vec::with_capacity(mint_config_txs.len());\n",
    "for tx in mint_config_txs {\n",
    "    // Ensure all nonces are unique.\n",
    "    if !seen_nonces.insert(tx.prefix.nonce.clone()) {\n",
    "        return Err(Error::FormBlock(format!(\n",
    "            \"Duplicate MintConfigTx nonce: {:?}\",\n",
    "            tx.prefix.nonce\n",
    "        )));\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "ai_report = analyze_and_fix_code(code_to_analyze)\n",
    "print(ai_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation against ground truth (tried to mimic human feedback using llama3):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Result:\n",
      "**Issue Identification:** 8/10\n",
      "The AI-generated output correctly identified eight issues with the original code, including Insecure Error Handling, Lack of Input Validation, Mutable State, Lack of Documentation, Unnecessary Allocation, Code Organization, Naming Conventions, and Memory Safety.\n",
      "\n",
      "However, it did not identify the specific issue mentioned in the ground truth: \"Nonces are not stored per token.\" This is a significant issue that affects the security of the code. Therefore, I deduct 2 points from the total score.\n",
      "\n",
      "**Problem Analysis:** 7/10\n",
      "The AI-generated output provides some analysis for each issue identified, but it does not go into as much detail as the ground truth. For example, the AI does not explain why storing nonces per token is important or how it affects security. Additionally, some of the issues mentioned in the AI-generated output are not directly related to the specific problem described in the ground truth.\n",
      "\n",
      "**Proposed Fixes:** 9/10\n",
      "The AI-generated output provides corrected code that addresses several of the issues identified, including Insecure Error Handling, Lack of Documentation, Unnecessary Allocation, and Code Organization. The proposed fixes are mostly accurate and relevant to the original code.\n",
      "\n",
      "However, the AI-generated output does not provide a fix for the specific issue mentioned in the ground truth: storing nonces per token. Therefore, I deduct 1 point from the total score.\n",
      "\n",
      "**Overall Accuracy:** 7/10\n",
      "The AI-generated output is mostly accurate and relevant, but it did not identify one of the most critical issues with the original code (storing nonces per token). Additionally, some of the proposed fixes may not be directly applicable to the specific problem described in the ground truth.\n",
      "\n",
      "Overall Score: 6.5/10\n",
      "\n",
      "**Explanation:** The AI-generated output is mostly accurate and relevant, but it did not identify one of the most critical issues with the original code (storing nonces per token). Additionally, some of the proposed fixes may not be directly applicable to the specific problem described in the ground truth.\n",
      "\n",
      "Discrepancies: The AI-generated output did not identify the issue mentioned in the ground truth regarding storing nonces per token.\n"
     ]
    }
   ],
   "source": [
    "def evaluate_against_ground_truth(ai_output, ground_truth):\n",
    "    evaluator = AIAgent(\n",
    "        model=\"llama3\",\n",
    "        system_prompt=\"\"\"You are an expert code reviewer and evaluator. Your task is to compare the AI-generated analysis and fixes with the ground truth provided. \n",
    "        Evaluate the accuracy, completeness, and relevance of the AI's output. \n",
    "        Provide a score from 0 to 10 for each of the following categories:\n",
    "        1. Issue Identification\n",
    "        2. Problem Analysis\n",
    "        3. Proposed Fixes\n",
    "        4. Overall Accuracy\n",
    "        \n",
    "        Also, provide a brief explanation for each score and any discrepancies found.\"\"\"\n",
    "    )\n",
    "\n",
    "    evaluation_prompt = f\"\"\"\n",
    "    Compare the following AI-generated output with the provided ground truth:\n",
    "\n",
    "    AI-generated output:\n",
    "    {ai_output}\n",
    "\n",
    "    Ground Truth:\n",
    "    {ground_truth}\n",
    "\n",
    "    Evaluate the AI output based on the criteria mentioned in the system prompt.\n",
    "    \"\"\"\n",
    "\n",
    "    evaluation = evaluator.get_response(evaluation_prompt)\n",
    "    return evaluation\n",
    "\n",
    "# Example usage\n",
    "code_to_analyze = \"\"\"\n",
    "let mut seen_nonces = BTreeSet::default();\n",
    "let mut validated_txs = Vec::with_capacity(mint_config_txs.len());\n",
    "for tx in mint_config_txs {\n",
    "    // Ensure all nonces are unique.\n",
    "    if !seen_nonces.insert(tx.prefix.nonce.clone()) {\n",
    "        return Err(Error::FormBlock(format!(\n",
    "            \"Duplicate MintConfigTx nonce: {:?}\",\n",
    "            tx.prefix.nonce\n",
    "        )));\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "ground_truth = \"\"\"\n",
    "Nonces are not stored per token\n",
    "Severity: Low \n",
    "Difficulty: High\n",
    "\n",
    "Mint and mint configuration transaction nonces are not distinguished by the tokens with\n",
    "which they are associated. Malicious minters or governors could use this fact to conduct\n",
    "denial-of-service attacks against other minters and governors.\n",
    "The relevant code appears in figures 4.1 and 4.2. For each type of transaction, nonces are\n",
    "inserted into a seen_nonces set without regard to the token indicated in the transaction.\n",
    "\n",
    "let mut seen_nonces = BTreeSet::default();\n",
    "let mut validated_txs = Vec::with_capacity(mint_config_txs.len());\n",
    "for tx in mint_config_txs {\n",
    "// Ensure all nonces are unique.\n",
    "if !seen_nonces.insert(tx.prefix.nonce.clone()) {\n",
    "return Err(Error::FormBlock(format!(\n",
    "\"Duplicate MintConfigTx nonce: {:?}\",\n",
    "tx.prefix.nonce\n",
    ")));\n",
    "}\n",
    "\n",
    "\n",
    "let mut mint_txs = Vec::with_capacity(mint_txs_with_config.len());\n",
    "let mut seen_nonces = BTreeSet::default();\n",
    "for (mint_tx, mint_config_tx, mint_config) in mint_txs_with_config {\n",
    "// The nonce should be unique.\n",
    "if !seen_nonces.insert(mint_tx.prefix.nonce.clone()) {\n",
    "return Err(Error::FormBlock(format!(\n",
    "\"Duplicate MintTx nonce: {:?}\",\n",
    "mint_tx.prefix.nonce\n",
    ")));\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Generate AI output\n",
    "# ai_output = analyze_and_fix_code(code_to_analyze)\n",
    "\n",
    "# Evaluate against ground truth\n",
    "evaluation_result = evaluate_against_ground_truth(ai_report, ground_truth)\n",
    "\n",
    "print(\"Evaluation Result:\")\n",
    "print(evaluation_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
